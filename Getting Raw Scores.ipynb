{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87c485e6-5f8e-443e-91fa-6b75b3c3d5d3",
   "metadata": {},
   "source": [
    "## Computing raw scores produces but different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14039ff8-45a0-44d8-a3b5-dcd0a9acde89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "124bdd0f-06d4-41ab-aeeb-ae24ea1a9e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "from datasets import Dataset\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48b0d9e9-9ddf-4a7c-9b90-999c8957dcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_raw(model_name, data):\n",
    "    with open(f'data/raw_scores_{model_name}.pkl', 'wb') as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1883667-1e09-4dcf-baaf-b648b054f0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unified function for hugging face models\n",
    "def compute_hf(model_name, short, df):\n",
    "    dataset = Dataset.from_pandas(df.reset_index()[['text']])\n",
    "    \n",
    "    model = pipeline(\"text-classification\", model = model_name, top_k = None, device = 'mps')\n",
    "\n",
    "    s = time.time()\n",
    "    scores = model(dataset['text'])\n",
    "\n",
    "    if len(scores) != len(df):\n",
    "        raise ValueError(f\"Length mismatch: df has {len(df)} rows but senti_scores has {len(scores)} elements\")\n",
    "\n",
    "    scores = {id_val: score for id_val, score in zip(df['id'], scores)}\n",
    "    \n",
    "    total = time.time() - s\n",
    "    print('Time', round(total))\n",
    "\n",
    "    save_raw(short, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8bc43ad-cc45-48af-af3c-ea8ebca8ce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified function to process a chunk and return scores\n",
    "def compute_hf_chunk(model, chunk_df):\n",
    "    dataset = Dataset.from_pandas(chunk_df.reset_index()[['text']])\n",
    "    \n",
    "    s = time.time()\n",
    "    scores = model(dataset['text'])\n",
    "    \n",
    "    if len(scores) != len(chunk_df):\n",
    "        raise ValueError(f\"Length mismatch: chunk_df has {len(chunk_df)} rows but scores has {len(scores)} elements\")\n",
    "    \n",
    "    scores_dict = {id_val: score for id_val, score in zip(chunk_df['id'], scores)}\n",
    "    \n",
    "    total = time.time() - s\n",
    "    print('Time', round(total))\n",
    "    \n",
    "    return scores_dict\n",
    "\n",
    "def compute_hf_chunks(model_name, short, df, chunk_size=100000):\n",
    "    total_rows = len(df)\n",
    "    total_chunks = (total_rows + chunk_size - 1) // chunk_size  # Ceiling division\n",
    "    all_scores = {}\n",
    "\n",
    "    model = pipeline(\"text-classification\", model=model_name, top_k=None, device='mps')\n",
    "    \n",
    "    # Process data in chunks\n",
    "    for chunk_index in range(total_chunks):\n",
    "        start_idx = chunk_index * chunk_size\n",
    "        end_idx = min((chunk_index + 1) * chunk_size, total_rows)\n",
    "        \n",
    "        print(f\"Processing chunk {chunk_index + 1}/{total_chunks} (rows {start_idx} to {end_idx-1})\")\n",
    "        \n",
    "        # Get current chunk\n",
    "        chunk_df = df.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # Process the chunk\n",
    "        chunk_scores = compute_hf_chunk(model, chunk_df)\n",
    "        \n",
    "        # Update all scores\n",
    "        all_scores.update(chunk_scores)\n",
    "        \n",
    "        # Save intermediate results\n",
    "        save_raw(f\"{short}_chunk_{chunk_index+1}\", chunk_scores)\n",
    "    \n",
    "    # Save the complete results\n",
    "    save_raw(short, all_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514c68c3-3d0e-443c-9dfe-9ed94d955284",
   "metadata": {},
   "source": [
    "## NRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "970825d4-ff79-4aa4-a1b4-d1039dbb28b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRC\n",
      "Time 280.5289590358734\n"
     ]
    }
   ],
   "source": [
    "from nrclex import NRCLex\n",
    "\n",
    "model_name = 'nrc'\n",
    "print('NRC')\n",
    "\n",
    "df = pd.read_csv('data/dataset.csv', quoting=1, escapechar='\\\\', doublequote=True)\n",
    "#df = df.head(100000)\n",
    "s = time.time()\n",
    "\n",
    "scores = {}\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    model = NRCLex(row['text'])\n",
    "    scores[row['id']] = model.top_emotions\n",
    "    \n",
    "total = time.time() - s\n",
    "print('Time', total)\n",
    "\n",
    "save_raw(model_name, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e60df6-4283-4cf1-85b1-25c2426fede6",
   "metadata": {},
   "source": [
    "## Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c19ddcb7-e45b-4490-a9cc-bf773ac92efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vader\n",
      "Time 154.49059987068176\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "vader_model = SentimentIntensityAnalyzer()\n",
    "\n",
    "model_name = 'vader'\n",
    "print('Vader')\n",
    "\n",
    "df = pd.read_csv('data/dataset.csv', quoting=1, escapechar='\\\\', doublequote=True)\n",
    "#df = df.head(100000)\n",
    "s = time.time()\n",
    "\n",
    "scores = {}\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    scores[row['id']] = vader_model.polarity_scores(row['text'])\n",
    "    \n",
    "total = time.time() - s\n",
    "print('Time', total)\n",
    "\n",
    "save_raw(model_name, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26369ccd-b5a2-440f-aefb-b49d773e7600",
   "metadata": {},
   "source": [
    "## Pysentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "300763b5-05bb-4101-818a-803b34f698e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysentimiento.preprocessing import preprocess_tweet\n",
    "from pysentimiento import create_analyzer\n",
    "#senti_model = create_analyzer(task=\"sentiment\", lang = 'en')\n",
    "emo_model = create_analyzer(task=\"emotion\", lang=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c6fb7547-c949-4abe-aa5f-1253411331dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pysentimiento (senti)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df3c7e53fbb945d8ad50fd5d4918fb5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1711514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 14852\n"
     ]
    }
   ],
   "source": [
    "model_name = 'pysentimiento_senti'\n",
    "print('Pysentimiento (senti)')\n",
    "\n",
    "df = pd.read_csv('data/dataset.csv', quoting=1, escapechar='\\\\', doublequote=True)\n",
    "#df = df.head(100000)\n",
    "\n",
    "s = time.time()\n",
    "senti_scores = senti_model.predict(df['text'])\n",
    "\n",
    "scores = {id_val: score for id_val, score in zip(df['id'], senti_scores)}\n",
    "\n",
    "total = time.time() - s\n",
    "print('Time', round(total))\n",
    "save_raw(model_name, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c30f816-58d0-4ed2-b016-d39a1b237e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pysentimiento (emo)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34ec0c301e42474dba9849a543cac068",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1711514 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = 'pysentimiento_emo'\n",
    "print('Pysentimiento (emo)')\n",
    "\n",
    "df = pd.read_csv('data/dataset.csv', quoting=1, escapechar='\\\\', doublequote=True)\n",
    "#df = df.head(100000)\n",
    "\n",
    "s = time.time()\n",
    "emo_scores = emo_model.predict(df['text'])\n",
    "\n",
    "scores = {id_val: score for id_val, score in zip(df['id'], emo_scores)}\n",
    "\n",
    "total = time.time() - s\n",
    "print('Time', round(total))\n",
    "save_raw(model_name, scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a42308-480a-4961-aca1-1e868dc04a62",
   "metadata": {},
   "source": [
    "## Hartmann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c4965c22-8115-4e49-908d-967719c7e541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hartmann\n",
      "Time 681\n"
     ]
    }
   ],
   "source": [
    "model_name = 'j-hartmann/emotion-english-distilroberta-base'\n",
    "print('Hartmann')\n",
    "df = pd.read_csv('data/dataset.csv', quoting=1, escapechar='\\\\', doublequote=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d8213b-db22-4c52-8b2d-7ecbd668b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(100000)\n",
    "compute_hf(model_name, 'hartmann', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "009cd004-4926-411d-a622-4f7a87e6ab6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hartmann\n",
      "Processing chunk 1/18 (rows 0 to 99999)\n",
      "Time 555\n",
      "Processing chunk 2/18 (rows 100000 to 199999)\n",
      "Time 2288\n",
      "Processing chunk 3/18 (rows 200000 to 299999)\n",
      "Time 503\n",
      "Processing chunk 4/18 (rows 300000 to 399999)\n",
      "Time 626\n",
      "Processing chunk 5/18 (rows 400000 to 499999)\n",
      "Time 505\n",
      "Processing chunk 6/18 (rows 500000 to 599999)\n",
      "Time 519\n",
      "Processing chunk 7/18 (rows 600000 to 699999)\n",
      "Time 510\n",
      "Processing chunk 8/18 (rows 700000 to 799999)\n",
      "Time 509\n",
      "Processing chunk 9/18 (rows 800000 to 899999)\n",
      "Time 513\n",
      "Processing chunk 10/18 (rows 900000 to 999999)\n",
      "Time 509\n",
      "Processing chunk 11/18 (rows 1000000 to 1099999)\n",
      "Time 532\n",
      "Processing chunk 12/18 (rows 1100000 to 1199999)\n",
      "Time 517\n",
      "Processing chunk 13/18 (rows 1200000 to 1299999)\n",
      "Time 499\n",
      "Processing chunk 14/18 (rows 1300000 to 1399999)\n",
      "Time 506\n",
      "Processing chunk 15/18 (rows 1400000 to 1499999)\n",
      "Time 509\n",
      "Processing chunk 16/18 (rows 1500000 to 1599999)\n",
      "Time 509\n",
      "Processing chunk 17/18 (rows 1600000 to 1699999)\n",
      "Time 504\n",
      "Processing chunk 18/18 (rows 1700000 to 1711513)\n",
      "Time 58\n"
     ]
    }
   ],
   "source": [
    "compute_hf_chunks(model_name, 'hartmann', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9b874a-8e12-41cc-ade8-25b19d3e7256",
   "metadata": {},
   "source": [
    "## Cardiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49e5834b-4524-4123-9637-6a252e9b6663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cardiff\n"
     ]
    }
   ],
   "source": [
    "model_name = 'cardiffnlp/twitter-roberta-base-sentiment-latest'\n",
    "print('Cardiff')\n",
    "df = pd.read_csv('data/dataset.csv', quoting=1, escapechar='\\\\', doublequote=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e7921c-4398-4dfa-9fa6-19eff64f1e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(100000)\n",
    "compute_hf(model_name, 'cardif', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a059d072-bcf5-40b2-af7f-f904eb327556",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/18 (rows 0 to 99999)\n",
      "Time 899\n",
      "Processing chunk 2/18 (rows 100000 to 199999)\n",
      "Time 863\n",
      "Processing chunk 3/18 (rows 200000 to 299999)\n",
      "Time 843\n",
      "Processing chunk 4/18 (rows 300000 to 399999)\n",
      "Time 826\n",
      "Processing chunk 5/18 (rows 400000 to 499999)\n",
      "Time 840\n",
      "Processing chunk 6/18 (rows 500000 to 599999)\n",
      "Time 835\n",
      "Processing chunk 7/18 (rows 600000 to 699999)\n",
      "Time 838\n",
      "Processing chunk 8/18 (rows 700000 to 799999)\n",
      "Time 834\n",
      "Processing chunk 9/18 (rows 800000 to 899999)\n",
      "Time 2188\n",
      "Processing chunk 10/18 (rows 900000 to 999999)\n",
      "Time 818\n",
      "Processing chunk 11/18 (rows 1000000 to 1099999)\n",
      "Time 854\n",
      "Processing chunk 12/18 (rows 1100000 to 1199999)\n",
      "Time 887\n",
      "Processing chunk 13/18 (rows 1200000 to 1299999)\n",
      "Time 892\n",
      "Processing chunk 14/18 (rows 1300000 to 1399999)\n",
      "Time 854\n",
      "Processing chunk 15/18 (rows 1400000 to 1499999)\n",
      "Time 857\n",
      "Processing chunk 16/18 (rows 1500000 to 1599999)\n",
      "Time 972\n",
      "Processing chunk 17/18 (rows 1600000 to 1699999)\n",
      "Time 849\n",
      "Processing chunk 18/18 (rows 1700000 to 1711513)\n",
      "Time 95\n"
     ]
    }
   ],
   "source": [
    "compute_hf_chunks(model_name, 'cardif', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719059cf-0f35-4cd7-8899-d14416f0ed94",
   "metadata": {},
   "source": [
    "## Siebert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98796ba4-9889-4746-9d7a-74959f86b55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Siebert\n"
     ]
    }
   ],
   "source": [
    "model_name = 'siebert/sentiment-roberta-large-english'\n",
    "print('Siebert')\n",
    "df = pd.read_csv('data/dataset.csv', quoting=1, escapechar='\\\\', doublequote=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c5c581-fbde-443c-a332-23ce9c82c99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(100000)\n",
    "compute_hf(model_name, 'siebert', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07c76aff-4f3c-4457-a1d9-7246cce4fde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/18 (rows 0 to 99999)\n",
      "Time 1891\n",
      "Processing chunk 2/18 (rows 100000 to 199999)\n",
      "Time 1873\n",
      "Processing chunk 3/18 (rows 200000 to 299999)\n",
      "Time 29154\n",
      "Processing chunk 4/18 (rows 300000 to 399999)\n",
      "Time 1781\n",
      "Processing chunk 5/18 (rows 400000 to 499999)\n",
      "Time 1797\n",
      "Processing chunk 6/18 (rows 500000 to 599999)\n",
      "Time 16084\n",
      "Processing chunk 7/18 (rows 600000 to 699999)\n",
      "Time 16484\n",
      "Processing chunk 8/18 (rows 700000 to 799999)\n",
      "Time 12824\n",
      "Processing chunk 9/18 (rows 800000 to 899999)\n",
      "Time 5783\n",
      "Processing chunk 10/18 (rows 900000 to 999999)\n",
      "Time 12954\n",
      "Processing chunk 11/18 (rows 1000000 to 1099999)\n",
      "Time 16190\n",
      "Processing chunk 12/18 (rows 1100000 to 1199999)\n",
      "Time 4335\n",
      "Processing chunk 13/18 (rows 1200000 to 1299999)\n",
      "Time 6724\n",
      "Processing chunk 14/18 (rows 1300000 to 1399999)\n",
      "Time 14920\n",
      "Processing chunk 15/18 (rows 1400000 to 1499999)\n",
      "Time 14647\n",
      "Processing chunk 16/18 (rows 1500000 to 1599999)\n",
      "Time 7155\n",
      "Processing chunk 17/18 (rows 1600000 to 1699999)\n",
      "Time 1841\n",
      "Processing chunk 18/18 (rows 1700000 to 1711513)\n",
      "Time 214\n"
     ]
    }
   ],
   "source": [
    "compute_hf_chunks(model_name, 'siebert', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425dbd40-1644-49b2-b59c-f552c05670ce",
   "metadata": {},
   "source": [
    "## LEIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1b114f4-360f-4745-a244-c3a817879654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEIA\n"
     ]
    }
   ],
   "source": [
    "model_name = 'LEIA/LEIA-large'\n",
    "print('LEIA')\n",
    "df = pd.read_csv('data/dataset.csv', quoting=1, escapechar='\\\\', doublequote=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2226f635-3039-4191-9deb-a071b96a2a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.head(100000)\n",
    "compute_hf(model_name, 'leia', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd738077-7509-4d6f-be3a-6c0722dd25f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 1/18 (rows 0 to 99999)\n",
      "Time 1772\n",
      "Processing chunk 2/18 (rows 100000 to 199999)\n",
      "Time 1775\n",
      "Processing chunk 3/18 (rows 200000 to 299999)\n",
      "Time 1810\n",
      "Processing chunk 4/18 (rows 300000 to 399999)\n",
      "Time 1759\n",
      "Processing chunk 5/18 (rows 400000 to 499999)\n",
      "Time 1756\n",
      "Processing chunk 6/18 (rows 500000 to 599999)\n",
      "Time 1772\n",
      "Processing chunk 7/18 (rows 600000 to 699999)\n",
      "Time 1747\n",
      "Processing chunk 8/18 (rows 700000 to 799999)\n",
      "Time 1762\n",
      "Processing chunk 9/18 (rows 800000 to 899999)\n",
      "Time 1817\n",
      "Processing chunk 10/18 (rows 900000 to 999999)\n",
      "Time 4524\n",
      "Processing chunk 11/18 (rows 1000000 to 1099999)\n",
      "Time 1853\n",
      "Processing chunk 12/18 (rows 1100000 to 1199999)\n",
      "Time 1856\n",
      "Processing chunk 13/18 (rows 1200000 to 1299999)\n",
      "Time 2867\n",
      "Processing chunk 14/18 (rows 1300000 to 1399999)\n",
      "Time 1812\n",
      "Processing chunk 15/18 (rows 1400000 to 1499999)\n",
      "Time 1842\n",
      "Processing chunk 16/18 (rows 1500000 to 1599999)\n",
      "Time 1864\n",
      "Processing chunk 17/18 (rows 1600000 to 1699999)\n",
      "Time 1825\n",
      "Processing chunk 18/18 (rows 1700000 to 1711513)\n",
      "Time 211\n"
     ]
    }
   ],
   "source": [
    "compute_hf_chunks(model_name, 'leia', df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6785969-758f-42a4-a579-e5f09366a596",
   "metadata": {},
   "source": [
    "## LIWC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa557318-ce39-4ec3-a1af-db8b668b5632",
   "metadata": {},
   "source": [
    "Done via LIWC software"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe1cc80-d51a-4bc4-83d0-66aef6c1f814",
   "metadata": {},
   "source": [
    "## LLMS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b952f0-0c42-458d-973c-6d0e4e5ab408",
   "metadata": {},
   "source": [
    "Results provided by Segun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a7d0ec-66f3-4250-b76b-a6229957b5dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
