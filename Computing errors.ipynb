{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03807951-6fd4-4331-a78c-fea72cba1f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import glob\n",
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np\n",
    "\n",
    "def open_final(model_name):\n",
    "    with open(f'data/final_scores_{model_name}.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0524c3c9-b035-498e-b658-153e8ea1b64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/dataset.csv', quoting=1, escapechar='\\\\', doublequote=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5ac416-dfc8-4f71-8b58-db7c9098cc0b",
   "metadata": {},
   "source": [
    "## Extending dataframe by adding emotions and sentiment based on mood labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dfc1e45-465a-4559-bc75-7d3da2aefeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "emomap = {'Sadness': ['Sad', 'Lonely'],\n",
    "         'Anger': ['Angry', 'Annoyed', 'Frustrated', 'Furious'],\n",
    "         'Fear': ['Anxious', 'Stressed', 'Afraid', 'Nervous', 'Worried'],\n",
    "         'Affection': ['Loving', 'Caring', 'Supportive'],\n",
    "         'Happiness': ['Happy', 'Excited']}\n",
    "\n",
    "emomap_r = {}\n",
    "for emo in emomap:\n",
    "    for label in emomap[emo]:\n",
    "        emomap_r[label] = emo\n",
    "\n",
    "sentimap = {'Sadness': 'negative', 'Anger': 'negative', 'Fear': 'negative', 'Affection': 'positive', 'Happiness': 'positive'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa2d39dd-e1b7-4a75-b617-22bd0c15e733",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emotion'] = df['mood'].map(emomap_r)\n",
    "df['sentiment'] = df['emotion'].map(sentimap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fcd6316-8a4a-465d-bf7e-909778015669",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/dataset_extended.csv', index=False, quoting=1, escapechar='\\\\', doublequote=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718a88db-c7c6-4a83-8060-043fce576362",
   "metadata": {},
   "source": [
    "## Computing errors\n",
    "Note that total error is not equal to valence error plus salience error. If for example the actual\n",
    "sentiment is negative but it is predicted to be positive in 33.3% of cases and neutral in 33.3% of cases\n",
    "then the valence error is 50%, the salience error is 50%, and the total error is 66.6%.\n",
    "The goal of this definition is to separate salience and valence errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7f8f28c5-6d8b-454f-baaa-866829cf6bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "import numpy as np\n",
    "\n",
    "def compute_valence_metrics_by_gender(non_neutral_df):\n",
    "    \"\"\"\n",
    "    Calculate valence error rates by gender and perform chi-square test\n",
    "    \"\"\"\n",
    "    # Initialize results\n",
    "    result = {\n",
    "        'valence_error_0': 0,  # Error rate for sex=0\n",
    "        'valence_error_1': 0,  # Error rate for sex=1\n",
    "        'valence_p': np.nan    # p-value for gender comparison\n",
    "    }\n",
    "    \n",
    "    if len(non_neutral_df) == 0:\n",
    "        return result\n",
    "    \n",
    "    # Create contingency table [gender, correct/incorrect]\n",
    "    contingency = np.zeros((2, 2))\n",
    "    \n",
    "    # Calculate error rates and fill contingency table for each gender\n",
    "    for i, sex_value in enumerate([0, 1]):\n",
    "        sex_df = non_neutral_df[non_neutral_df['sex'] == sex_value]\n",
    "        \n",
    "        # Calculate error rate for this gender\n",
    "        if len(sex_df) > 0:\n",
    "            error_rate = sum(sex_df['is_valence_error']) / len(sex_df)\n",
    "            result[f'valence_error_{sex_value}'] = error_rate\n",
    "            \n",
    "            # Fill contingency table\n",
    "            # Correct predictions (column 0)\n",
    "            contingency[i, 0] = sum(~sex_df['is_valence_error'])\n",
    "            # Incorrect predictions (column 1)\n",
    "            contingency[i, 1] = sum(sex_df['is_valence_error'])\n",
    "    \n",
    "    # Perform chi-square test if possible\n",
    "    if np.all(contingency > 0):\n",
    "        _, p_value, _, _ = chi2_contingency(contingency)\n",
    "        result['valence_p'] = p_value\n",
    "    elif np.any(contingency > 0):\n",
    "        result['valence_p'] = 1.0\n",
    "    \n",
    "    return result\n",
    "\n",
    "def compute_salience_metrics_by_gender(potential_correct_df):\n",
    "    \"\"\"\n",
    "    Calculate salience error rates by gender and perform chi-square test\n",
    "    \"\"\"\n",
    "    # Initialize results\n",
    "    result = {\n",
    "        'salience_error_0': 0,  # Error rate for sex=0\n",
    "        'salience_error_1': 0,  # Error rate for sex=1\n",
    "        'salience_p': np.nan    # p-value for gender comparison\n",
    "    }\n",
    "    \n",
    "    if len(potential_correct_df) == 0:\n",
    "        return result\n",
    "    \n",
    "    # Create contingency table [gender, non-neutral/neutral]\n",
    "    contingency = np.zeros((2, 2))\n",
    "    \n",
    "    # Calculate error rates and fill contingency table for each gender\n",
    "    for i, sex_value in enumerate([0, 1]):\n",
    "        sex_df = potential_correct_df[potential_correct_df['sex'] == sex_value]\n",
    "        \n",
    "        # Calculate error rate for this gender\n",
    "        if len(sex_df) > 0:\n",
    "            error_rate = sum(sex_df['is_neutral_pred']) / len(sex_df)\n",
    "            result[f'salience_error_{sex_value}'] = error_rate\n",
    "            \n",
    "            # Fill contingency table\n",
    "            # Non-neutral predictions (column 0)\n",
    "            contingency[i, 0] = sum(~sex_df['is_neutral_pred'])\n",
    "            # Neutral predictions (column 1)\n",
    "            contingency[i, 1] = sum(sex_df['is_neutral_pred'])\n",
    "    \n",
    "    # Perform chi-square test if possible\n",
    "    if np.all(contingency > 0):\n",
    "        _, p_value, _, _ = chi2_contingency(contingency)\n",
    "        result['salience_p'] = p_value\n",
    "    elif np.any(contingency > 0):\n",
    "        result['salience_p'] = 1.0\n",
    "    \n",
    "    return result\n",
    "\n",
    "def compute_errors_by_gender(df, scores):\n",
    "    \"\"\"\n",
    "    Compute valence and salience errors by gender with statistical comparison\n",
    "    \"\"\"\n",
    "    # Filter df to only include rows where id is in scores\n",
    "    valid_df = df[df['id'].isin(scores.keys())].copy()\n",
    "    \n",
    "    # If no valid rows, return empty results\n",
    "    if len(valid_df) == 0:\n",
    "        return {\n",
    "            'valence_error_0': 0,\n",
    "            'valence_error_1': 0,\n",
    "            'valence_p': np.nan,\n",
    "            'salience_error_0': 0,\n",
    "            'salience_error_1': 0,\n",
    "            'salience_p': np.nan\n",
    "        }\n",
    "    \n",
    "    # Add predictions to the dataframe\n",
    "    valid_df['predicted'] = valid_df['id'].map(scores)\n",
    "    \n",
    "    # Create flags for different types of predictions\n",
    "    valid_df['is_neutral_pred'] = valid_df['predicted'] == 'neutral'\n",
    "    valid_df['is_valence_error'] = (valid_df['sentiment'] != valid_df['predicted']) & (~valid_df['is_neutral_pred'])\n",
    "    \n",
    "    # Calculate valence metrics\n",
    "    non_neutral_df = valid_df[~valid_df['is_neutral_pred']]\n",
    "    valence_results = compute_valence_metrics_by_gender(non_neutral_df)\n",
    "    \n",
    "    # Calculate salience metrics\n",
    "    potential_correct_df = valid_df[~valid_df['is_valence_error']]\n",
    "    salience_results = compute_salience_metrics_by_gender(potential_correct_df)\n",
    "    \n",
    "    # Combine results\n",
    "    return {**valence_results, **salience_results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3f87a752-c0b2-4dd4-bdc0-30630763d683",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_group_errors(df, scores):\n",
    "    # Create empty list to store results\n",
    "    result_data = []\n",
    "    \n",
    "    # Define the levels and their possible values\n",
    "    levels = {\n",
    "        'sentiment': ['positive', 'negative'],\n",
    "        'emotion': df['emotion'].unique().tolist(),  # Get all unique emotion values\n",
    "        'mood': df['mood'].unique().tolist()         # Get all unique mood values\n",
    "    }\n",
    "    \n",
    "    # Iterate through levels (sentiment, emotion, mood)\n",
    "    for level, level_values in levels.items():\n",
    "        # Iterate through each possible value within the level\n",
    "        for level_value in level_values:\n",
    "            # Filter the dataframe for this level value\n",
    "            df_filtered = df[df[level] == level_value]\n",
    "            \n",
    "            # Skip if empty\n",
    "            if df_filtered.empty:\n",
    "                continue\n",
    "                \n",
    "            # Compute errors with gender comparison for this subset\n",
    "            errors = compute_errors_by_gender(df_filtered, scores)\n",
    "            \n",
    "            # Add valence error for each gender\n",
    "            for sex_value in [0, 1]:\n",
    "                # Add valence error\n",
    "                result_data.append({\n",
    "                    'sex': sex_value,\n",
    "                    'level': level,\n",
    "                    'level_value': level_value,\n",
    "                    'error_type': 'valence',\n",
    "                    'error_value': errors[f'valence_error_{sex_value}'],\n",
    "                    'p_value': errors['valence_p']\n",
    "                })\n",
    "                \n",
    "                # Add salience error\n",
    "                result_data.append({\n",
    "                    'sex': sex_value,\n",
    "                    'level': level,\n",
    "                    'level_value': level_value,\n",
    "                    'error_type': 'salience',\n",
    "                    'error_value': errors[f'salience_error_{sex_value}'],\n",
    "                    'p_value': errors['salience_p']\n",
    "                })\n",
    "    \n",
    "    # Create DataFrame from the collected data\n",
    "    result_df = pd.DataFrame(result_data)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd928fa-2601-4702-8802-1abf91ec9602",
   "metadata": {},
   "source": [
    "Note that 0 is man and 1 is woman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "88b2d473-ebf8-4429-b845-683796671624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/final_scores_vader.pkl\n",
      "data/final_scores_olmo.pkl\n",
      "data/final_scores_hartman.pkl\n",
      "data/final_scores_falcon.pkl\n",
      "data/final_scores_pysentimiento_senti.pkl\n",
      "data/final_scores_hartmann.pkl\n",
      "data/final_scores_leia.pkl\n",
      "data/final_scores_cardif.pkl\n",
      "data/final_scores_llama8.pkl\n",
      "data/final_scores_siebert.pkl\n",
      "data/final_scores_mistral.pkl\n",
      "data/final_scores_granite.pkl\n",
      "data/final_scores_pysentimiento_emo.pkl\n",
      "data/final_scores_nrc.pkl\n",
      "data/final_scores_qwen.pkl\n",
      "data/final_scores_liwc.pkl\n",
      "data/final_scores_llama70.pkl\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/dataset_extended.csv', quoting=1, escapechar='\\\\', doublequote=True)\n",
    "\n",
    "model_files = glob.glob('data/final_scores_*.pkl')\n",
    "\n",
    "result = []\n",
    "\n",
    "for model_file in model_files:\n",
    "    print(model_file)\n",
    "    model_name = model_file.split('final_scores_')[1].split('.pkl')[0]\n",
    "    \n",
    "    scores = open_final(model_name)\n",
    "    \n",
    "    errors = compute_group_errors(df, scores)\n",
    "    errors['model'] = model_name\n",
    "    \n",
    "    result.append(errors)\n",
    "\n",
    "result = pd.concat(result, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60980306-dc29-4cc3-9ec6-3118b40c5c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_types = {\n",
    "    'dictionary': ['liwc', 'nrc', 'vader'],\n",
    "    'llm': ['mistral', 'falcon', 'llama8', 'llama70', 'olmo', 'qwen', 'granite'],\n",
    "    'ml': ['cardif', 'hartmann', 'leia', 'pysentimiento_emo', 'pysentimiento_senti', 'siebert']\n",
    "}\n",
    "\n",
    "model_type_map = {}\n",
    "for model_type, models in model_types.items():\n",
    "    for model in models:\n",
    "        model_type_map[model] = model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1359faa4-4a1a-4374-84ee-5134860b0d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['model_type'] = result['model'].map(model_type_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9791163-0099-42d5-ad2c-f1c288ebd1ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>level</th>\n",
       "      <th>level_value</th>\n",
       "      <th>error_type</th>\n",
       "      <th>error_value</th>\n",
       "      <th>p_value</th>\n",
       "      <th>model</th>\n",
       "      <th>model_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>positive</td>\n",
       "      <td>valence</td>\n",
       "      <td>0.179742</td>\n",
       "      <td>1.488615e-197</td>\n",
       "      <td>vader</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>positive</td>\n",
       "      <td>salience</td>\n",
       "      <td>0.227838</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>vader</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>positive</td>\n",
       "      <td>valence</td>\n",
       "      <td>0.138240</td>\n",
       "      <td>1.488615e-197</td>\n",
       "      <td>vader</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>positive</td>\n",
       "      <td>salience</td>\n",
       "      <td>0.170416</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>vader</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>sentiment</td>\n",
       "      <td>negative</td>\n",
       "      <td>valence</td>\n",
       "      <td>0.417493</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>vader</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sex      level level_value error_type  error_value        p_value  model  \\\n",
       "0    0  sentiment    positive    valence     0.179742  1.488615e-197  vader   \n",
       "1    0  sentiment    positive   salience     0.227838   0.000000e+00  vader   \n",
       "2    1  sentiment    positive    valence     0.138240  1.488615e-197  vader   \n",
       "3    1  sentiment    positive   salience     0.170416   0.000000e+00  vader   \n",
       "4    0  sentiment    negative    valence     0.417493   0.000000e+00  vader   \n",
       "\n",
       "   model_type  \n",
       "0  dictionary  \n",
       "1  dictionary  \n",
       "2  dictionary  \n",
       "3  dictionary  \n",
       "4  dictionary  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "16337933-7825-45ce-a0e3-7e01c9f016a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results for LLMs are computed on limited sample size and is not part of preregistration\n",
    "result.loc[result['model_type'] == 'llm', 'p_value'] = np.nan\n",
    "\n",
    "# Salience error is available only for a few models\n",
    "result.loc[result['error_type'] == 'salience', 'p_value'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c5792984-fd37-47a3-ba1a-bfce0bc24fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(result[result['level'] == 'mood']['level_value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6f59471c-7784-45c6-a7d9-c1b700b7c32a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qn/r236hk7d6g9d5m5jhyy4tqmc0000gn/T/ipykernel_18111/3602865633.py:9: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[ True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True  True  True  True  True  True  True  True  True\n",
      "  True  True  True  True]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  result.loc[mask_sentiment, 'significance'] = result.loc[mask_sentiment, 'p_value'] < 0.001\n"
     ]
    }
   ],
   "source": [
    "result['significance'] = np.nan\n",
    "\n",
    "# When p_value is not NA, compute significance based on level\n",
    "mask_valid_p = result['p_value'].notna()\n",
    "\n",
    "#Compute significance using pre-registered thresholds:\n",
    "\n",
    "mask_sentiment = (mask_valid_p) & (result['level'] == 'sentiment')\n",
    "result.loc[mask_sentiment, 'significance'] = result.loc[mask_sentiment, 'p_value'] < 0.001\n",
    "\n",
    "mask_emotion = (mask_valid_p) & (result['level'] == 'emotion')\n",
    "result.loc[mask_emotion, 'significance'] = result.loc[mask_emotion, 'p_value'] < (0.001 / 5)\n",
    "\n",
    "mask_mood = (mask_valid_p) & (result['level'] == 'mood')\n",
    "result.loc[mask_mood, 'significance'] = result.loc[mask_mood, 'p_value'] < (0.001 / 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3d4e9b73-c3a2-4092-9c70-d979de2532f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('data/errors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3d524cf5-f80e-4603-833f-0fcd62edb1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute basic stat\n",
    "# Removing not relevant rows and considering only one gender as p-values are the same\n",
    "stats = result[(mask_valid_p) & (result['sex'] == 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4c0534ad-c20c-49a3-8db8-dea084329b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment\n",
      "Men bias 20 / 20\n",
      "Women bias 0 / 20\n",
      "Not significant 0 / 20\n",
      "\n",
      "emotion\n",
      "Men bias 44 / 50\n",
      "Women bias 0 / 50\n",
      "Not significant 6 / 50\n",
      "\n",
      "mood\n",
      "Men bias 115 / 160\n",
      "Women bias 0 / 160\n",
      "Not significant 45 / 160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for level in ['sentiment', 'emotion', 'mood']:\n",
    "    print(level)\n",
    "    level_data = stats[stats['level'] == level]\n",
    "    total = len(level_data)\n",
    "    non_sig = len(level_data[level_data['significance'] == False])\n",
    "    sign_data = level_data[level_data['significance'] == True]\n",
    "    men_bias = len(sign_data[sign_data['error_value'] > 0])\n",
    "    women_bias = len(sign_data[sign_data['error_value'] < 0])\n",
    "    print('Men bias', men_bias, '/', total)\n",
    "    print('Women bias', women_bias, '/', total)\n",
    "    print('Not significant', non_sig, '/', total)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
